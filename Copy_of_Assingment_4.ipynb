{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Assingment 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-N-Martin/GreenBottle/blob/master/Copy_of_Assingment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z6yb_gzz1XI",
        "colab_type": "text"
      },
      "source": [
        "# Assignmnet 4\n",
        "\n",
        "### IFT6758 Fall 2019\n",
        "\n",
        "### Due date: December 15, 2019\n",
        "\n",
        "### Submit your answers and code as a pdf file in gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzOxCc2oz1XK",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning (Convolutional Neural Networks) [60 points + 10 bonus points]\n",
        "\n",
        "This set of assignments will give you experience with deep learning. You will learn how to use convolution neural networks on a image corpus. \n",
        "\n",
        "For this problem use [documentation for Keras](https://keras.io/) deep learning library and for [Sklearn](https://scikit-learn.org/stable/index.html). Provide your code and the output.\n",
        "\n",
        "#### Data preparation (20 points)\n",
        "\n",
        "1. (1 point) Search MNIST dataset at [OpenML](https://www.openml.org/), it is called \"mnist_784\". Download it using sklearn function `fetch_openml`. Get features and targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y98XeYcTneRz",
        "colab_type": "text"
      },
      "source": [
        "**Note that this notebook with the code and answer is accessible at https://colab.research.google.com/drive/1gaK9edV7YExQg2PAjAY5fGaUKklUHXp0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6V5LFCXct6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import sklearn.datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRSF6Kdbz1XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your solution here\n",
        "X,y = sklearn.datasets.fetch_openml('mnist_784', return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___LmoL7z1XN",
        "colab_type": "text"
      },
      "source": [
        "> This dataset represents each (28x28) image as a flat arrray of 784 features.\n",
        "\n",
        "2. (3 point) Reshape it back to 28x28 and visualize a couple of images with matplotlib.\n",
        "\n",
        "> Finally, we want to add one dummy dimension for the non-existent color information. Every resulting image should have dimensions 28x28x1. \n",
        ">\n",
        "> We want this extra dimensions because image libraries are targeted towards RGB images that have 3 channels. Therefore RGB images are conviniently represented by the shape of WxHx3. We don't have 3 colors for MNIST dataset, so we just provide 1 channel.\n",
        "\n",
        "3. (3 point) Add a channel dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Im01is2z1XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "reshape_X = np.array([np.reshape(sample,(28,28,1)) for sample in X])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn9MhHtSz1XP",
        "colab_type": "text"
      },
      "source": [
        "> To simplify the task we exclude some classes.\n",
        "\n",
        "4. (3 point) Filter data leaving only classes `1`, `3`, `7`. Transform features and targets. How many data points left after filtering?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKgguIQz1XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind = [i for i in range(len(y)) if y[i] =='1' or y[i]=='3' or y[i]=='7']\n",
        "filter_X = reshape_X[ind,:]\n",
        "filter_y = y[ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM6cEzfftNnu",
        "colab_type": "code",
        "outputId": "73b49856-6a4d-4c5d-a4a6-08f3abc1fbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "filter_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22311, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcN2AjvtS42",
        "colab_type": "text"
      },
      "source": [
        "**Answer:** We have only 22,311 data points after filtering instead of 70,000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5V6yFByz1XR",
        "colab_type": "text"
      },
      "source": [
        "5. (5 point) Convert targets to one-hot representation. Complete the following template.\n",
        "\n",
        "\n",
        "```python\n",
        "def to_categorical(array, classes):\n",
        "    \"\"\"\n",
        "    array -- array of targets\n",
        "    classes -- list of classes\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "mnist_targets = to_categorical(mnist_targets, classes=...)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wcXJ9pRz1XS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_categorical(array, classes):\n",
        "    \"\"\"\n",
        "    array -- array of targets\n",
        "    classes -- list of classes\n",
        "    \"\"\"\n",
        "    dict_of_classes = {}\n",
        "    for i,cl in enumerate(classes):\n",
        "      dict_of_classes[cl]=i\n",
        "    onehot = np.zeros((len(array),len(classes)))\n",
        "    for i in range(len(array)):\n",
        "      onehot[i][dict_of_classes[array[i]]]=1\n",
        "    \n",
        "    return onehot\n",
        "\n",
        "    \n",
        " \n",
        "mnist_targets = to_categorical(filter_y, classes=[cl for cl in np.unique(filter_y)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k1JoH3m3lwT",
        "colab_type": "code",
        "outputId": "39a39c05-9a93-4012-ce43-d78a6072e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "mnist_targets "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFSmQf2az1XT",
        "colab_type": "text"
      },
      "source": [
        "6. (5 point) Split the dataset into train, validataion, and test. Take first 16,000 images and targets as the train, then next 3,000 as validation, then the rest as the test subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOshw8xxz1XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = filter_X[:16000]\n",
        "y_train = mnist_targets[:16000]\n",
        "X_valid = filter_X[16000:19000]\n",
        "y_valid = mnist_targets[16000:19000]\n",
        "X_test = filter_X[19000:]\n",
        "y_test = mnist_targets[19000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbd6JDHFz1XW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Training (35 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwaDGjhz1XW",
        "colab_type": "text"
      },
      "source": [
        "Use Keras (https://keras.io/) to create a neural network model. Use a sequential layer to combine following layers in this order:\n",
        "- Convolution with 6 feature maps 5x5\n",
        "- Rectified linear unit activation\n",
        "- Max-pooling by factor of 2 each spacial dimension\n",
        "- Convolution with 16 feature maps 5x5\n",
        "- Rectified linear unit activation\n",
        "- Max-pooling by factor of 2 each spacial dimension\n",
        "- Flatten layer\n",
        "- Dense layer with 128 output units\n",
        "- Rectified linear unit activation\n",
        "- Dense layer. Same size as the target.\n",
        "- Softmax activation\n",
        "\n",
        "1. (10 points) Complete the following template.\n",
        "\n",
        "```python\n",
        "... # place your imports here\n",
        "\n",
        "model = Sequential([\n",
        "    ..., # convolution\n",
        "    ..., # activation\n",
        "    ..., # pooling\n",
        "    ..., # convolution\n",
        "    ..., # activation\n",
        "    ..., # pooling\n",
        "    Flatten(),\n",
        "    ..., # fully connected\n",
        "    ..., # activation\n",
        "    ..., # fully connected output\n",
        "    ..., # softmax\n",
        "])\n",
        "model.summary()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvRbp25Uz1XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSydTJuT2JVZ",
        "colab_type": "code",
        "outputId": "157a44e5-2790-4189-fe7b-365cee2227e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(6, (5, 5), input_shape=(28,28,1)), # convolution\n",
        "    Activation('relu'), # activation\n",
        "    MaxPooling2D(pool_size=(2,2)), # pooling\n",
        "    Conv2D(16, (5, 5)), # convolution\n",
        "    Activation('relu'), # activation\n",
        "    MaxPooling2D(pool_size=(2,2)), # pooling\n",
        "    Flatten(),\n",
        "    Dense(128), # fully connected\n",
        "    Activation('relu'), # activation\n",
        "    Dense(3), # fully connected output\n",
        "    Activation('softmax'), # softmax\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 6)         156       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 24, 24, 6)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 16)          2416      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 387       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 35,855\n",
            "Trainable params: 35,855\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5mn_1G-z1XZ",
        "colab_type": "text"
      },
      "source": [
        "2. (5 point) Create a stochastic gradient optimizer optimizer with learning rate of $10^{-4}$. Compile the model with the categorical crossentropy loss. Set the model to report accuracy metric. Complete the template.\n",
        "\n",
        "\n",
        "```python\n",
        "... # place your imports here\n",
        "\n",
        "optimizer = ... # create stochastic gradient optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'],\n",
        "              )\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hX9AsDfz1XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        " \n",
        "optimizer = optimizers.SGD(lr=0.0001) # create stochastic gradient optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'],\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4CJxXqVz1Xb",
        "colab_type": "text"
      },
      "source": [
        "3. (15 points) Train the model on the training set for at least 5 epochs. Perform validation after every epoch.\n",
        "\n",
        "> **HINT** Find a method that performs training in the Keras documentation. Study the documentation paying attention to all arguments and the return value of the method.\n",
        "\n",
        "> The model should have at least 95% accuracy on the training set. It might happen that the training gets stuck. In this case, go to the step before prevoious, recreate and rerun the model. \n",
        "\n",
        "> **WARNING** This step might take several minutes to compute on a laptop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIBpRiD0z1Xb",
        "colab_type": "code",
        "outputId": "2c513f32-abcd-49c9-8537-53a22da169cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "hist = model.fit(x=X_train,y=y_train,epochs=25,validation_data=(X_valid,y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 3000 samples\n",
            "Epoch 1/25\n",
            "16000/16000 [==============================] - 10s 622us/step - loss: 1.6961 - acc: 0.8633 - val_loss: 0.2519 - val_acc: 0.9707\n",
            "Epoch 2/25\n",
            "16000/16000 [==============================] - 10s 596us/step - loss: 0.2974 - acc: 0.9661 - val_loss: 0.1860 - val_acc: 0.9767\n",
            "Epoch 3/25\n",
            "16000/16000 [==============================] - 9s 587us/step - loss: 0.2266 - acc: 0.9731 - val_loss: 0.1637 - val_acc: 0.9807\n",
            "Epoch 4/25\n",
            "16000/16000 [==============================] - 10s 595us/step - loss: 0.1837 - acc: 0.9774 - val_loss: 0.1369 - val_acc: 0.9820\n",
            "Epoch 5/25\n",
            "16000/16000 [==============================] - 9s 584us/step - loss: 0.1558 - acc: 0.9802 - val_loss: 0.1262 - val_acc: 0.9813\n",
            "Epoch 6/25\n",
            "16000/16000 [==============================] - 9s 587us/step - loss: 0.1352 - acc: 0.9825 - val_loss: 0.1304 - val_acc: 0.9813\n",
            "Epoch 7/25\n",
            "16000/16000 [==============================] - 9s 592us/step - loss: 0.1185 - acc: 0.9845 - val_loss: 0.1184 - val_acc: 0.9827\n",
            "Epoch 8/25\n",
            "16000/16000 [==============================] - 9s 584us/step - loss: 0.1086 - acc: 0.9857 - val_loss: 0.1043 - val_acc: 0.9850\n",
            "Epoch 9/25\n",
            "16000/16000 [==============================] - 9s 584us/step - loss: 0.0982 - acc: 0.9872 - val_loss: 0.0989 - val_acc: 0.9857\n",
            "Epoch 10/25\n",
            "16000/16000 [==============================] - 9s 594us/step - loss: 0.0906 - acc: 0.9876 - val_loss: 0.0923 - val_acc: 0.9857\n",
            "Epoch 11/25\n",
            "16000/16000 [==============================] - 10s 629us/step - loss: 0.0825 - acc: 0.9883 - val_loss: 0.0836 - val_acc: 0.9873\n",
            "Epoch 12/25\n",
            "16000/16000 [==============================] - 10s 631us/step - loss: 0.0752 - acc: 0.9891 - val_loss: 0.0924 - val_acc: 0.9853\n",
            "Epoch 13/25\n",
            "16000/16000 [==============================] - 10s 622us/step - loss: 0.0697 - acc: 0.9889 - val_loss: 0.0853 - val_acc: 0.9860\n",
            "Epoch 14/25\n",
            "16000/16000 [==============================] - 10s 618us/step - loss: 0.0650 - acc: 0.9896 - val_loss: 0.0680 - val_acc: 0.9900\n",
            "Epoch 15/25\n",
            "16000/16000 [==============================] - 10s 609us/step - loss: 0.0579 - acc: 0.9893 - val_loss: 0.0686 - val_acc: 0.9890\n",
            "Epoch 16/25\n",
            "16000/16000 [==============================] - 10s 638us/step - loss: 0.0519 - acc: 0.9921 - val_loss: 0.0654 - val_acc: 0.9900\n",
            "Epoch 17/25\n",
            "16000/16000 [==============================] - 10s 619us/step - loss: 0.0487 - acc: 0.9919 - val_loss: 0.0618 - val_acc: 0.9903\n",
            "Epoch 18/25\n",
            "16000/16000 [==============================] - 10s 627us/step - loss: 0.0463 - acc: 0.9919 - val_loss: 0.0603 - val_acc: 0.9897\n",
            "Epoch 19/25\n",
            "16000/16000 [==============================] - 10s 618us/step - loss: 0.0420 - acc: 0.9926 - val_loss: 0.0592 - val_acc: 0.9900\n",
            "Epoch 20/25\n",
            "16000/16000 [==============================] - 10s 597us/step - loss: 0.0393 - acc: 0.9929 - val_loss: 0.0550 - val_acc: 0.9900\n",
            "Epoch 21/25\n",
            "16000/16000 [==============================] - 10s 624us/step - loss: 0.0345 - acc: 0.9935 - val_loss: 0.0585 - val_acc: 0.9897\n",
            "Epoch 22/25\n",
            "16000/16000 [==============================] - 10s 627us/step - loss: 0.0337 - acc: 0.9934 - val_loss: 0.0538 - val_acc: 0.9887\n",
            "Epoch 23/25\n",
            "16000/16000 [==============================] - 10s 633us/step - loss: 0.0321 - acc: 0.9932 - val_loss: 0.0535 - val_acc: 0.9900\n",
            "Epoch 24/25\n",
            "16000/16000 [==============================] - 10s 628us/step - loss: 0.0294 - acc: 0.9943 - val_loss: 0.0602 - val_acc: 0.9893\n",
            "Epoch 25/25\n",
            "16000/16000 [==============================] - 10s 645us/step - loss: 0.0272 - acc: 0.9946 - val_loss: 0.0635 - val_acc: 0.9890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZUHX0dz1Xd",
        "colab_type": "text"
      },
      "source": [
        "4. (5 points) Plot the training loss against the validation loss. Do you observe overfitting/underfitting?\n",
        "\n",
        "> **HINT** Explore the return value in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E02CbYsBz1Xd",
        "colab_type": "code",
        "outputId": "e8da456d-5b3e-4649-8483-26a4fb2384ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoiIlEWBISIiZVFgiIhIWf4/jysMukEs3UgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRqIv9xZz1Xf",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation (5 points)\n",
        "\n",
        "1. (1 point) Make prediciton of the model on the test set \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmv0rOkyz1Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prob = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKNg0W0Bz1Xh",
        "colab_type": "text"
      },
      "source": [
        "2. (4 points) Compute the confusion matrix and the accuracy. Which classes confused most often?\n",
        "\n",
        "> The model should have at least 90% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEUqHFXi2Xqd",
        "colab_type": "text"
      },
      "source": [
        "**Answer:** The two classes that get confused most often are the 3 and 7 with a total of 22 misclassified samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr9y8D59z1Xi",
        "colab_type": "code",
        "outputId": "ede6324f-e380-4bd8-d2d8-b2446a68b24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(sklearn.metrics.confusion_matrix(y_test.argmax(axis=1),prob.argmax(axis=1),labels=[0, 1, 2]))\n",
        "print('The accuracy of the model is: ',sklearn.metrics.accuracy_score(y_test.argmax(axis=1),prob.argmax(axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1175    5    2]\n",
            " [   4 1042    7]\n",
            " [  16   15 1045]]\n",
            "The accuracy of the model is:  0.985200845665962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_RXshgLz1Xk",
        "colab_type": "text"
      },
      "source": [
        "#### Bonus point (10 points)\n",
        "\n",
        "Can you suggest an improvement to the model? Implement it and compare to the one above. How to do robust comparison of the performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-IWL4Y4ZcW",
        "colab_type": "text"
      },
      "source": [
        "**Answer:** We could increase the number of feature maps to detect more pattern in the image. To compare the two models, we should train both model multiple times and ensuring that we use the same seed for both model each time and then, compare their overall average performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sej8L9EU2zHO",
        "colab_type": "code",
        "outputId": "b69ed47c-a920-4292-9ee9-a910545e6c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "model_bonus = Sequential([\n",
        "    Conv2D(36, (5, 5), input_shape=(28,28,1)), # convolution\n",
        "    Activation('relu'), # activation\n",
        "    MaxPooling2D(pool_size=(2,2)), # pooling\n",
        "    Conv2D(46, (5, 5)), # convolution\n",
        "    Activation('relu'), # activation\n",
        "    MaxPooling2D(pool_size=(2,2)), # pooling\n",
        "    Flatten(),\n",
        "    Dense(128), # fully connected\n",
        "    Activation('relu'), # activation\n",
        "    Dense(3), # fully connected output\n",
        "    Activation('softmax'), # softmax\n",
        "])\n",
        "model_bonus.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 36)        936       \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 24, 24, 36)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 12, 12, 36)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 8, 8, 46)          41446     \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 8, 8, 46)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 4, 4, 46)          0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 736)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 128)               94336     \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 3)                 387       \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 137,105\n",
            "Trainable params: 137,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjRwprsV3Xk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimizers.SGD(lr=0.0005) # create stochastic gradient optimizer\n",
        "model_bonus.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'],\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYjj-JkE3bV1",
        "colab_type": "code",
        "outputId": "c0e832a6-26c5-417a-80ca-3adae86e72f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "hist_bonus = model_bonus.fit(x=X_train,y=y_train,epochs=25,validation_data=(X_valid,y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 3000 samples\n",
            "Epoch 1/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 5.2462 - acc: 0.6706 - val_loss: 5.2567 - val_acc: 0.6727\n",
            "Epoch 2/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.8814 - acc: 0.9356 - val_loss: 0.0716 - val_acc: 0.9897\n",
            "Epoch 3/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0829 - acc: 0.9874 - val_loss: 0.0423 - val_acc: 0.9923\n",
            "Epoch 4/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0452 - acc: 0.9923 - val_loss: 0.0341 - val_acc: 0.9940\n",
            "Epoch 5/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0273 - acc: 0.9949 - val_loss: 0.0224 - val_acc: 0.9943\n",
            "Epoch 6/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0169 - acc: 0.9963 - val_loss: 0.0237 - val_acc: 0.9957\n",
            "Epoch 7/25\n",
            "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0127 - acc: 0.9971 - val_loss: 0.0218 - val_acc: 0.9943\n",
            "Epoch 8/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0086 - acc: 0.9987 - val_loss: 0.0235 - val_acc: 0.9950\n",
            "Epoch 9/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0227 - val_acc: 0.9947\n",
            "Epoch 10/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0276 - val_acc: 0.9930\n",
            "Epoch 11/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0038 - acc: 0.9995 - val_loss: 0.0210 - val_acc: 0.9950\n",
            "Epoch 12/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0203 - val_acc: 0.9960\n",
            "Epoch 13/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0206 - val_acc: 0.9957\n",
            "Epoch 14/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0203 - val_acc: 0.9953\n",
            "Epoch 15/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0200 - val_acc: 0.9957\n",
            "Epoch 16/25\n",
            "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0202 - val_acc: 0.9957\n",
            "Epoch 17/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0202 - val_acc: 0.9953\n",
            "Epoch 18/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.0203 - val_acc: 0.9957\n",
            "Epoch 19/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0201 - val_acc: 0.9953\n",
            "Epoch 20/25\n",
            "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0199 - val_acc: 0.9953\n",
            "Epoch 21/25\n",
            "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0198 - val_acc: 0.9957\n",
            "Epoch 22/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0197 - val_acc: 0.9957\n",
            "Epoch 23/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0199 - val_acc: 0.9957\n",
            "Epoch 24/25\n",
            "16000/16000 [==============================] - 21s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0197 - val_acc: 0.9957\n",
            "Epoch 25/25\n",
            "16000/16000 [==============================] - 22s 1ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0198 - val_acc: 0.9957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4KHD7k_GPD",
        "colab_type": "code",
        "outputId": "6a817d25-a479-4e72-8230-ae5d1f858042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#First model: Accuracy of 98.52%\n",
        "prob_bonus = model_bonus.predict(X_test)\n",
        "print('The accuracy of the model is: ',sklearn.metrics.accuracy_score(y_test.argmax(axis=1),prob_bonus.argmax(axis=1)))\n",
        "#Second model: Accuracy of 99.3%"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the model is:  0.9930534581697372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSPkmqIZz1Xk",
        "colab_type": "text"
      },
      "source": [
        "### Graph ML [40 points]\n",
        "\n",
        "This set of assingments will teach you the differences between various node representations in graphs. Note that all questions are programming assingments but you do not need to use loss function to optimize the claculation of thesee embeddings.  \n",
        "\n",
        "1- (5 points) Write a function randadjmat(n,p) in Python which returns an adjacency matrix for a \"random graph\" on n vertices. Here p is the probability of having an edge between any pair of vertices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaDQ8fGem84d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LerbKcHLn1EX",
        "colab_type": "text"
      },
      "source": [
        "The following function will create an adjacency matrix for a random undirected graph where there is no egdes between a node and itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeQCdczuz1Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randadjmat(n,p):\n",
        "    adjmat = np.zeros((n,n))\n",
        "    for r in range(n):\n",
        "      for c in range(r,n):\n",
        "        prob = random.uniform(0,1)\n",
        "        if prob<p and c!=r:\n",
        "          adjmat[r,c]=1\n",
        "          adjmat[c,r]=1 \n",
        "    return adjmat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5czQc_GOmy_L",
        "colab_type": "code",
        "outputId": "fa0bf3ff-9b5f-41bb-c5dd-14a78b93120a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "mat = randadjmat(4,.6)\n",
        "print(mat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1.]\n",
            " [0. 0. 1. 1.]\n",
            " [0. 1. 0. 1.]\n",
            " [1. 1. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Ayg9Sxpei-",
        "colab_type": "text"
      },
      "source": [
        "We can make a different function where we could specify which type of graph we want instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VibALp8jpd0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randadjmat2(n,p,type_graph):\n",
        "    adjmat = np.zeros((n,n))\n",
        "    if type_graph == 'undirected':\n",
        "      for r in range(n):\n",
        "        for c in range(r,n):\n",
        "          prob = random.uniform(0,1)\n",
        "          if prob<p:\n",
        "            adjmat[r,c]=1\n",
        "            adjmat[c,r]=1\n",
        "      return adjmat\n",
        "    if type_graph =='directed':\n",
        "      for r in range(n):\n",
        "        for c in range(n):\n",
        "          prob = random.uniform(0,1)\n",
        "          if prob<p:\n",
        "            adjmat[r,c] =1\n",
        "      return adjmat\n",
        "    else:\n",
        "      return \"The only acceptable types are 'directed' and 'undirected'.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP2gGUwFpd_2",
        "colab_type": "code",
        "outputId": "6abc7a01-b5aa-4625-9be6-ad43641e4eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "matdir = randadjmat2(4,.6,'directed')\n",
        "print(matdir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 1.]\n",
            " [1. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx5xxqfdz1Xn",
        "colab_type": "text"
      },
      "source": [
        "2- (5 points) Write a function transionmat(A) which, given an adjacency matrix A, generate a transition matrix T where probability of each edge (u,v) is calculated as $1/degree(u)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDwHKejWz1Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transionmat(A):\n",
        "    return A/A.sum(axis=1, keepdims=True)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaOUts9vs6u6",
        "colab_type": "code",
        "outputId": "38e39667-bdd9-4f6a-9598-2704d223272d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "tran = transionmat(mat)\n",
        "print(tran)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         1.        ]\n",
            " [0.         0.         0.5        0.5       ]\n",
            " [0.         0.5        0.         0.5       ]\n",
            " [0.33333333 0.33333333 0.33333333 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iwkaLWyz1Xp",
        "colab_type": "text"
      },
      "source": [
        "3- (5 points) Write a function hotembd(A) which, given an adjacency matrix A, generate an embedding matrix H where each node is represetned with a 1-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK5qonBUz1Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hotembd(A):\n",
        "    return np.eye(len(A))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgZGBOEX1zpL",
        "colab_type": "code",
        "outputId": "62ced80f-1de6-4973-be55-c01e5dee8125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "hot = hotembd(matdir)\n",
        "print(hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj327X0Fz1Xr",
        "colab_type": "text"
      },
      "source": [
        "4- (5 points) Write a function randwalkemb(A,k) which, given an adjacency matrix A, a transition matrix T, and one-hot encoding H, performs [random walks](https://en.wikipedia.org/wiki/Random_walk) on the graph from each node w times with lenght equal to l and generate an embedding matrix for each node based on the sum of 1-hot encodings of all nodes that are visited during the walks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8uJ5bE_z1Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randwalkembd(A,T, H, w, l):\n",
        "    final_mat = np.zeros(shape=(A.shape))\n",
        "    cum_prob = np.cumsum(T,axis=1)\n",
        "    for node in range(len(A)):\n",
        "      for n_walk in range(w):\n",
        "        #print('New walk starts')\n",
        "        current_node = node\n",
        "        for step in range(l):\n",
        "          #print('We take the step #',step+1)\n",
        "          prob = random.uniform(0,1)\n",
        "          futur_node = np.argwhere(cum_prob[current_node,:]>prob)[0]\n",
        "          if len(futur_node)>1:\n",
        "            next_node = futur_node[-1]\n",
        "          else: \n",
        "            next_node=futur_node\n",
        "          #print(node,current_node,next_node)\n",
        "          #print(H[next_node])\n",
        "          final_mat[node] = final_mat[node] + H[next_node]\n",
        "          #print(final_mat)\n",
        "          current_node=next_node\n",
        "    \n",
        "    return final_mat\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdK41biK7ZlU",
        "colab_type": "code",
        "outputId": "090d45ce-3f26-46c0-e9d8-a9a237670268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "embrand = randwalkembd(mat,tran,hot,2,3)\n",
        "print(embrand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 2. 3.]\n",
            " [0. 1. 3. 2.]\n",
            " [0. 3. 2. 1.]\n",
            " [0. 3. 1. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LmVTdbbz1Xu",
        "colab_type": "text"
      },
      "source": [
        "5- (5 points) Write a function hopeneighbormbd(A,H,k) which, given an adjacency matrix A, and one-hot node encoding matrix H, generates node embedding matrix which represents each node as sum of 1-hot encodings of k-hobs neighbors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78ER544z1Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hopeneighbormbd(A, H, k):\n",
        "    return np.linalg.matrix_power(A,k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTI-7W7mz1Xz",
        "colab_type": "text"
      },
      "source": [
        "6- (5 points) Write a function similarnodes(Z) which, given an node embedding matrix, find the most similar nodes in the graph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu4IdRM-z1Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarnodes(Z):\n",
        "    sim = np.dot(np.transpose(Z),Z)\n",
        "    np.fill_diagonal(sim,0)\n",
        "    ind = np.unravel_index(np.argmax(sim, axis=None), sim.shape)\n",
        "    return ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyCoww7i5AXp",
        "colab_type": "code",
        "outputId": "ce9db53a-d8c4-4668-b290-0fc00eef1a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "similarnodes(embrand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxScaoM6z1X1",
        "colab_type": "text"
      },
      "source": [
        "7- (10 points) generate a random graph where n=20, and p=0.6, and compare the most similar nodes in the graph using randwalkembd (l=4, w=10), hopeneighbormbd (k=1) and hopeneighbormbd (k=2). Justify why similar nodes are different using different node embeddings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgtqzptbz1X1",
        "colab_type": "code",
        "outputId": "c35edb9f-4aa6-4e62-be55-b4f4d4e9e704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## We generate the adjencency matrix of a random graph using randadjmat:\n",
        "A = randadjmat(20,.6)\n",
        "\n",
        "print('The matrix is:', A)\n",
        "\n",
        "## We can create our transition matrix and our one-hot matrix related to A.\n",
        "T = transionmat(A)\n",
        "H = hotembd(A)\n",
        "\n",
        "print(\"The random walk embedding:\")\n",
        "embed_walk = randwalkembd(A,T,H,10,4)\n",
        "print(embed_walk)\n",
        "print(\"The 1-hop embedding:\")\n",
        "embed_hop1 = hopeneighbormbd(A, H, 1)\n",
        "print(embed_hop1)\n",
        "print(\"The 2-hop embedding:\")\n",
        "embed_hop2 = hopeneighbormbd(A, H, 2)\n",
        "print(embed_hop2)\n",
        "\n",
        "print(\"The two nodes that are most similar with the random walk embedding are:\", similarnodes(embed_walk)[0] , \"and\", similarnodes(embed_walk)[1],\".\" )\n",
        "print(\"The two nodes that are most similar with the 1-hop embedding are:\", similarnodes(embed_hop1)[0], \"and\",similarnodes(embed_hop1)[1], \".\"  )\n",
        "print(\"The two nodes that are most similar with the 2-hop embedding are:\", similarnodes(embed_hop2)[0], \"and\",similarnodes(embed_hop2)[1], \".\"  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The matrix is: [[0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
            " [0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.]]\n",
            "The random walk embedding:\n",
            "[[3. 3. 1. 4. 1. 2. 6. 0. 2. 1. 2. 2. 2. 2. 2. 2. 1. 4. 0. 0.]\n",
            " [2. 0. 1. 1. 1. 0. 2. 0. 2. 3. 4. 3. 1. 3. 3. 3. 0. 4. 2. 5.]\n",
            " [3. 1. 2. 3. 1. 0. 2. 0. 2. 3. 5. 0. 3. 2. 1. 0. 3. 4. 3. 2.]\n",
            " [2. 1. 0. 0. 0. 1. 2. 0. 2. 3. 2. 3. 5. 1. 1. 3. 4. 4. 1. 5.]\n",
            " [4. 2. 0. 1. 3. 2. 2. 1. 3. 3. 0. 3. 3. 1. 0. 2. 1. 2. 4. 3.]\n",
            " [3. 3. 1. 1. 2. 0. 6. 2. 0. 4. 0. 0. 2. 0. 0. 6. 1. 3. 4. 2.]\n",
            " [5. 2. 1. 2. 1. 2. 2. 2. 2. 0. 2. 3. 1. 2. 2. 1. 2. 3. 3. 2.]\n",
            " [1. 0. 0. 0. 3. 2. 5. 0. 3. 1. 4. 3. 2. 2. 1. 2. 5. 4. 0. 2.]\n",
            " [3. 1. 0. 2. 3. 1. 0. 1. 7. 3. 2. 2. 2. 5. 2. 0. 2. 2. 2. 0.]\n",
            " [2. 1. 0. 1. 2. 2. 2. 2. 5. 1. 2. 2. 5. 2. 1. 2. 1. 1. 4. 2.]\n",
            " [2. 1. 3. 1. 2. 0. 3. 5. 0. 1. 3. 2. 0. 1. 3. 3. 0. 2. 3. 5.]\n",
            " [2. 2. 0. 2. 3. 2. 4. 6. 1. 0. 1. 2. 1. 1. 3. 3. 2. 2. 1. 2.]\n",
            " [1. 4. 1. 1. 4. 1. 3. 0. 3. 5. 1. 4. 1. 1. 3. 2. 2. 1. 1. 1.]\n",
            " [1. 2. 5. 1. 2. 1. 1. 1. 3. 1. 1. 0. 2. 2. 1. 2. 4. 3. 3. 4.]\n",
            " [3. 4. 0. 0. 3. 1. 4. 2. 4. 3. 1. 5. 1. 0. 2. 1. 0. 3. 2. 1.]\n",
            " [1. 4. 0. 2. 2. 2. 1. 5. 1. 1. 0. 3. 2. 1. 2. 4. 1. 2. 3. 3.]\n",
            " [3. 2. 3. 1. 2. 3. 2. 2. 1. 0. 2. 0. 5. 0. 1. 1. 5. 3. 1. 3.]\n",
            " [3. 1. 0. 1. 3. 2. 3. 2. 3. 1. 2. 1. 2. 4. 0. 3. 0. 1. 4. 4.]\n",
            " [2. 1. 0. 1. 2. 1. 1. 3. 3. 3. 3. 1. 4. 5. 0. 2. 2. 0. 1. 5.]\n",
            " [5. 1. 2. 1. 1. 1. 4. 0. 1. 1. 3. 2. 2. 1. 4. 0. 4. 5. 1. 1.]]\n",
            "The 1-hop embedding:\n",
            "[[0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
            " [0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.]]\n",
            "The 2-hop embedding:\n",
            "[[13.  8.  7.  4.  6.  7. 10.  6. 11.  7.  6. 10.  9.  7.  6.  9.  8.  8.\n",
            "   8.  9.]\n",
            " [ 8. 13.  4.  6.  9.  5.  9.  8.  9.  8.  7.  7.  8.  6.  6.  5.  7. 10.\n",
            "   9.  9.]\n",
            " [ 7.  4.  8.  4.  4.  4.  5.  6.  8.  4.  4.  5.  7.  3.  4.  5.  4.  4.\n",
            "   7.  6.]\n",
            " [ 4.  6.  4.  8.  6.  3.  6.  5.  6.  6.  7.  3.  6.  4.  5.  4.  4.  6.\n",
            "   6.  7.]\n",
            " [ 6.  9.  4.  6. 13.  6. 10.  7.  7.  8.  7.  6. 10.  8.  5.  8.  5. 10.\n",
            "  10. 10.]\n",
            " [ 7.  5.  4.  3.  6.  8.  5.  4.  6.  5.  3.  6.  7.  6.  3.  6.  5.  6.\n",
            "   7.  6.]\n",
            " [10.  9.  5.  6. 10.  5. 15.  6.  9.  7.  9. 10. 10.  9.  7.  8.  8.  9.\n",
            "   8. 12.]\n",
            " [ 6.  8.  6.  5.  7.  4.  6. 10.  8.  6.  6.  5.  7.  3.  5.  4.  3.  7.\n",
            "   9.  7.]\n",
            " [11.  9.  8.  6.  7.  6.  9.  8. 13.  7.  6.  8.  9.  5.  6.  8.  6.  7.\n",
            "  10.  9.]\n",
            " [ 7.  8.  4.  6.  8.  5.  7.  6.  7. 12.  7.  6.  9.  7.  5.  7.  5. 11.\n",
            "   8. 10.]\n",
            " [ 6.  7.  4.  7.  7.  3.  9.  6.  6.  7. 11.  6.  7.  6.  6.  5.  6.  9.\n",
            "   5.  9.]\n",
            " [10.  7.  5.  3.  6.  6. 10.  5.  8.  6.  6. 12.  9.  8.  6.  7.  7.  7.\n",
            "   6.  9.]\n",
            " [ 9.  8.  7.  6. 10.  7. 10.  7.  9.  9.  7.  9. 14.  8.  6.  8.  6.  9.\n",
            "  10. 11.]\n",
            " [ 7.  6.  3.  4.  8.  6.  9.  3.  5.  7.  6.  8.  8. 11.  6.  7.  6.  8.\n",
            "   6. 10.]\n",
            " [ 6.  6.  4.  5.  5.  3.  7.  5.  6.  5.  6.  6.  6.  6.  9.  5.  4.  5.\n",
            "   6.  8.]\n",
            " [ 9.  5.  5.  4.  8.  6.  8.  4.  8.  7.  5.  7.  8.  7.  5. 11.  5.  8.\n",
            "   8.  8.]\n",
            " [ 8.  7.  4.  4.  5.  5.  8.  3.  6.  5.  6.  7.  6.  6.  4.  5. 10.  8.\n",
            "   5.  9.]\n",
            " [ 8. 10.  4.  6. 10.  6.  9.  7.  7. 11.  9.  7.  9.  8.  5.  8.  8. 14.\n",
            "   9. 11.]\n",
            " [ 8.  9.  7.  6. 10.  7.  8.  9. 10.  8.  5.  6. 10.  6.  6.  8.  5.  9.\n",
            "  13. 10.]\n",
            " [ 9.  9.  6.  7. 10.  6. 12.  7.  9. 10.  9.  9. 11. 10.  8.  8.  9. 11.\n",
            "  10. 16.]]\n",
            "The two nodes that are most similar with the random walk embedding are: 6 and 17 .\n",
            "The two nodes that are most similar with the 1-hop embedding are: 6 and 19 .\n",
            "The two nodes that are most similar with the 2-hop embedding are: 6 and 19 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHwkSJAOxDXl",
        "colab_type": "text"
      },
      "source": [
        "**The nodes are different because they do not consider the same characteristic for similarity. Random walk consider two nodes similar if they happen together on a random walk while multi-hop encoding consider the link between the nodes to calculate similarity and in particular, the 1-hop consider only the nodes that are directly connected to each other.**"
      ]
    }
  ]
}